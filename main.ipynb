{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b40659",
   "metadata": {},
   "source": [
    "# Flamingo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee95a48",
   "metadata": {},
   "source": [
    "## å‚è€ƒ\n",
    "\n",
    "- [Flamingo: a Visual Language Model for Few-Shot Learning][0]\n",
    "- [OpenFlamingo: An Open-SOurce Framework for Training Large Autogressive Vision-Language Models][1]\n",
    "- [mlfoundations / open_flamingo][2]\n",
    "\n",
    "[0]: https://arxiv.org/abs/2204.14198\n",
    "[1]: https://arxiv.org/abs/2308.01390\n",
    "[2]: https://github.com/mlfoundations/open_flamingo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51687e3a",
   "metadata": {},
   "source": [
    "## æ¦‚è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c735a4",
   "metadata": {},
   "source": [
    "Flamingoã¯ã€å°‘æ•°ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒ«ã§æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«è¿…é€Ÿã«å¯¾å¿œã§ãã‚‹è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVisual Language Model, VLMï¼‰\n",
    "\n",
    "äº‹å‰å­¦ç¿’æ¸ˆã¿ã®è¦–è¦šãƒ¢ãƒ‡ãƒ«ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é€£æºã—ã€ç”»åƒãƒ»å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆã®å…¥åŠ›ã«å¯¾å¿œã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨\n",
    "\n",
    "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰åé›†ã—ãŸå¤§è¦æ¨¡ãªãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒãŒæ··åœ¨ã™ã‚‹ã‚³ãƒ¼ãƒ‘ã‚¹ã§è¨“ç·´\n",
    "\n",
    "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å›ç­”ä¾‹ã‚’å«ã‚ã¦æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«æŸ”è»Ÿã«å¯¾å¿œã™ã‚‹å­¦ç¿’ã‚’å®Ÿç¾ï¼ˆin-context few-shot learningï¼‰\n",
    "\n",
    "è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è©³ç´°ã«è©•ä¾¡:\n",
    "\n",
    "1. ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰ãªã‚¿ã‚¹ã‚¯ï¼ˆVisual Question-Answering, VQAï¼‰\n",
    "2. ç”»åƒã‚„å‹•ç”»ã«ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ã¤ã‘ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "3. ç”»åƒã«é–¢ã™ã‚‹å¤šè‚¢é¸æŠå¼ã®è³ªå•ã«å›ç­”ã™ã‚‹ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ã‚¨ãƒ³ãƒ‰ãªã‚¿ã‚¹ã‚¯\n",
    "\n",
    "å¤šãã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd113a",
   "metadata": {},
   "source": [
    "Flaming-80Bã§ã®ãƒ•ãƒ¥ãƒ¼ã‚·ãƒ§ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å‡ºåŠ›ã‚µãƒ³ãƒ—ãƒ«:\n",
    "\n",
    "![](image/fig1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e874ec9",
   "metadata": {},
   "source": [
    "è¤‡æ•°ã®ç”»åƒã«ã‚ãŸã‚‹å¯¾è©±ã‚‚å¯èƒ½:\n",
    "\n",
    "![](image/fig1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58998f24",
   "metadata": {},
   "source": [
    "FlamingoãŒ16ã®ã†ã¡6ã¤ã®ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ä¾‹ç¤ºæ•°ãŒå¢—ãˆã‚‹ã¨æ€§èƒ½ãŒä¸ŠãŒã‚‹:\n",
    "\n",
    "![](image/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1de92e",
   "metadata": {},
   "source": [
    "## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d17694",
   "metadata": {},
   "source": [
    "Flamingoã¯ã€2ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å›ºå®šã—ãŸï¼ˆfrozenï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨:\n",
    "\n",
    "1. è¦–è¦šãƒ¢ãƒ‡ãƒ«ï¼ˆVision Encoderï¼‰: ç”»åƒã‚„å‹•ç”»ã‚’çŸ¥è¦šã™ã‚‹\n",
    "2. è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLM blockï¼‰: åŸºæœ¬çš„ãªæ¨è«–ã‚’è¡Œã†\n",
    "\n",
    "äº‹å‰å­¦ç¿’ä¸­ã«ç²å¾—ã—ãŸçŸ¥è­˜ã‚’ä¿æŒã™ã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:\n",
    "\n",
    "- Perceiver Resampler: å¯å¤‰ãªé«˜è§£åƒåº¦ã®ç”»åƒã‚„å‹•ç”»ã‚’ã€å°‘æ•°ã®å›ºå®šã•ã‚ŒãŸè¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã™ã‚‹\n",
    "- ã‚²ãƒ¼ãƒˆä»˜ãã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯†çµåˆï¼ˆGATED XATTN-DENSEï¼‰: è¦–è¦šãƒ¢ãƒ‡ãƒ«ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ©‹æ¸¡ã—ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7293f43",
   "metadata": {},
   "source": [
    "![](image/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c56af3",
   "metadata": {},
   "source": [
    "Flamingoã¯ã€ç”Ÿæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ $y$ ã®å°¤åº¦ã‚’æ¬¡å¼ã§ãƒ¢ãƒ‡ãƒ«åŒ–:\n",
    "\n",
    "$$\n",
    "p(y|x) = \\prod_{l=1}^L p(y_l | y_{\\le l}, x_{\\le l})\n",
    "$$\n",
    "\n",
    "- ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®ç”Ÿæˆç¢ºç‡ $p(y|x)$ ã¯ã€ä»Šã¾ã§ã«è¦‹ãŸå…¨ã¦ã®è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ $x_{\\le l}$ ã¨ä»Šã¾ã§ã«ç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ $y_{\\le l}$ ã®æ¡ä»¶ä¸‹ã§æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ $y_l$ ãŒç”Ÿæˆã•ã‚Œã‚‹ç¢ºç‡ã‚’ã€æœ€åˆã‹ã‚‰æœ€å¾Œã¾ã§ã‹ã‘åˆã‚ã›ãŸå€¤\n",
    "- $y_l$: ãƒ†ã‚­ã‚¹ãƒˆ $y$ ã® $l$ ç•ªç›®ã®è¨€èªãƒˆãƒ¼ã‚¯ãƒ³\n",
    "- $y_{\\le l}$: $l$ ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚ˆã‚Šå‰ã«ã‚ã‚‹å…¨ã¦ã®è¨€èªãƒˆãƒ¼ã‚¯ãƒ³\n",
    "- $x_{\\le l}$: $l$ ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå‡¦ç†ã•ã‚Œã‚‹ã‚ˆã‚Šå‰ã®ã™ã¹ã¦ã®ç”»åƒã‚„å‹•ç”»ã®é›†åˆ\n",
    "- $p(\\cdot)$: ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦æ¨è«–ã•ã‚Œã‚‹ç¢ºç‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5696",
   "metadata": {},
   "source": [
    "è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ç‰¹å¾´:\n",
    "\n",
    "- Normalizer-Free ResNetï¼ˆNFNetï¼‰ã‚’ä½¿ç”¨\n",
    "- CLIPã¨åŒæ§˜ã®å¯¾è±¡å­¦ç¿’æå¤±ï¼ˆtwo-term contrastive lossï¼‰ã§äº‹å‰å­¦ç¿’\n",
    "- è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å‡ºåŠ›ã¯1æ¬¡å…ƒã«ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã•ã‚Œã‚‹\n",
    "- å‹•ç”»å…¥åŠ›ã®å ´åˆã€1FPSã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ç‹¬ç«‹ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€å­¦ç¿’æ¸ˆã¿ã®æ™‚é–“åŸ‹ã‚è¾¼ã¿ã‚’åŠ ç®—ã—ãƒ•ãƒ©ãƒƒãƒˆåŒ–\n",
    "- NFNetã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã‚ˆã†ãªCNNã®å ´åˆã€æš—é»™çš„ã«ç©ºé–“æƒ…å ±ã‚’ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨ã«å«ã‚“ã§ã„ã‚‹ãŸã‚ç”»åƒã®ä½ç½®åŸ‹ã‚è¾¼ã¿ã¯å¿…è¦ãªã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd7de1",
   "metadata": {},
   "source": [
    "Perceiver Resamplerã®ç‰¹å¾´:\n",
    "\n",
    "- è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ã®å¯å¤‰é•·ãªå…¥åŠ›ã‚’ã€å›ºå®šé•·ï¼ˆ=64ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆlatent input queriesï¼‰ã«å¤‰æ›\n",
    "- Perceiver Resamplerã¯ã€å­¦ç¿’å¯èƒ½ãªæ½œåœ¨ã‚¯ã‚¨ãƒªï¼ˆ=64ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’å…¥åŠ›ã¨ã—ãŸã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã™ã‚‹è¤‡æ•°ã®Transformerã§æ§‹æˆã•ã‚Œã‚‹\n",
    "\n",
    "![](image/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cca4b",
   "metadata": {},
   "source": [
    "ã‚²ãƒ¼ãƒˆä»˜ãã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ãƒ‰å¯†çµåˆï¼ˆGATED XATTN-DENSEï¼‰ã®ç‰¹å¾´ï¼š\n",
    "\n",
    "- ã‚²ãƒ¼ãƒˆä»˜ãã®ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã¨ã‚²ãƒ¼ãƒˆä»˜ããƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã§æ§‹æˆã•ã‚Œã‚‹\n",
    "- äº‹å‰å­¦ç¿’æ¸ˆã¿ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡çµã—ã€å…ƒã®å±¤ã®é–“ã«GATED XATTN-DENSEã‚’è¿½åŠ ã—ã€ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ã™ã‚‹\n",
    "- ã‚²ãƒ¼ãƒˆã¯tanh-gatingï¼ˆãƒã‚¤ãƒ‘ãƒœãƒªãƒƒã‚¯ã‚¿ãƒ³ã‚¸ã‚§ãƒ³ãƒˆã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰ã‚’æ¡ç”¨ã—ã€ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "- è¨“ç·´ã®æœ€åˆã¯å…ƒã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¨å¤‰ã‚ã‚‰ãªã„ãŸã‚å­¦ç¿’ãŒå®‰å®šåŒ–ã—ã€ãã®çµæœæœ€çµ‚çš„ãªçµæœã‚‚æ”¹å–„\n",
    "\n",
    "![](image/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bdbd6",
   "metadata": {},
   "source": [
    "è¨€èªãƒ¢ãƒ‡ãƒ«ã¯Chinchillaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã€3ã¤ã®ã‚µã‚¤ã‚ºã§å®Ÿé¨“:\n",
    "\n",
    "- 1.4Bï¼ˆFlamingo-3Bï¼‰\n",
    "- 7Bï¼ˆFlamingo-9Bï¼‰\n",
    "- 70Bï¼ˆFlamingo-80Bï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d844544f",
   "metadata": {},
   "source": [
    "ç”»åƒå› æœãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼ˆimage-causal modelingï¼‰ã®ä»•çµ„ã¿:\n",
    "\n",
    "- ç”»åƒå› æœãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¯ã€ä»Šã¾ã§å‡ºç¾ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã§ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”ŸæˆãŒæ¡ä»¶ä»˜ã‘ã‚‰ã‚Œã‚‹æ€§è³ª\n",
    "- ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã«ãƒã‚¹ã‚­ãƒ³ã‚°ã—ã€ã‚¢ãƒ†ãƒ³ãƒ‰ã§ãã‚‹è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã€Œç›´å‰ã«å‡ºç¾ã—ãŸç”»åƒã®ã¿ã€ã«åˆ¶é™ã™ã‚‹ã“ã¨ã§å®Ÿç¾\n",
    "- ã“ã®å˜ä¸€ç”»åƒã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆsingle-image cross attentionï¼‰ã«ã‚ˆã‚Šã€è¤‡æ•°æšç”»åƒå…¥åŠ›ã«æ±åŒ–å¯èƒ½ã«ãªã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc2e55",
   "metadata": {},
   "source": [
    "è¨“ç·´ã¯3ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ··ãœåˆã‚ã›ã¦å®Ÿæ–½:\n",
    "\n",
    "1. ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãŒäº¤äº’ã«é…ç½®ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã§æ§‹æˆã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    - 4300ä¸‡ä»¶ã®ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã—ã€HTMLæ§‹é€ ã‚’ã‚‚ã¨ã«å†æ§‹æˆã—ãŸM3Wï¼ˆMultiModal Massive Webï¼‰ã‚’ä½œæˆ\n",
    "    - ç”»åƒã®ä½ç½®ã«`<image>`ã‚¿ã‚°ã‚’è¿½åŠ ã—ã€å„ç”»åƒã®ç›´å‰ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæœ«å°¾ã«`<EOC>`ï¼ˆend of chunkï¼‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "    - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã« $L=256$ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€$N = 5$ æšã¾ã§ã®ç”»åƒã‚’ä½¿ç”¨\n",
    "2. ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒšã‚¢ã§æ§‹æˆã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    - 18å„„ã®ç”»åƒã¨altãƒ†ã‚­ã‚¹ãƒˆãŒãƒšã‚¢ã«ãªã£ãŸALIGNãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ™ãƒ¼ã‚¹ã«ä½¿ç”¨\n",
    "    - æ›´ã«ã‚ˆã‚Šé«˜å“è³ªã§é•·ã„èª¬æ˜æ–‡ã‚’å«ã‚€3å„„1200ä¸‡ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—è£œå®Œï¼ˆLTIP, Long Text & Image Pairsï¼‰\n",
    "3. å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒšã‚¢ã§æ§‹æˆã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    - å¹³å‡ç´„22ç§’ã®çŸ­ã„å‹•ç”»ã¨èª¬æ˜æ–‡ãŒãƒšã‚¢ã«ãªã£ãŸ2700ä¸‡ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã§æ§‹æˆï¼ˆVTP, Video & Text Pairsï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bf0c0",
   "metadata": {},
   "source": [
    "è¨“ç·´ã®æå¤±é–¢æ•°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ã«é‡ã¿ä»˜ã‘ã•ã‚ŒãŸæœŸå¾…è² å¯¾æ•°å°¤åº¦ï¼ˆexpected negative log-liklihoodï¼‰ã‚’ä½¿ç”¨:\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^M \\lambda_m \\cdot \\mathbb{E}_{(x,y) \\sim D_m} \\left[ - \\sum_{l=1}^L \\log p(y_l | y_{\\lt l}, x_{\\le l}) \\right]\n",
    "$$\n",
    "\n",
    "- å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ$D_m$ï¼‰ã«ã¤ã„ã¦ã€è² ã®å¯¾æ•°å°¤åº¦ã®å¹³å‡ï¼ˆ$\\mathbb{E}$ï¼‰ã‚’è¨ˆç®—ã—ã€å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é‡è¦åº¦ $\\lambda_m$ ã§é‡ã¿ä»˜ã‘ã—åˆè¨ˆã—ãŸå€¤\n",
    "- $D_m$: $m$ ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "- $\\lambda_m$: $m$ ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é‡è¦åº¦\n",
    "\n",
    "è¨“ç·´ã§ã¯ã€å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒãƒƒãƒã‚’è¨ˆç®—ã—ã€ãã‚Œãã‚Œã®å‹¾é…ã‚’å€‹åˆ¥ã«è¨ˆç®—ã—ã€åˆç®—ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ï¼ˆå¤šç›®çš„è¨“ç·´ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e7059",
   "metadata": {},
   "source": [
    "è¨“ç·´å¾Œã¯ã€ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆin-context learningï¼‰ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¹ã‚¯é©å¿œåŠ›ã‚’è©•ä¾¡:\n",
    "\n",
    "- ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãŒãƒšã‚¢ã«ãªã£ãŸå›ç­”ä¾‹ï¼ˆsupport example pairsï¼‰ã®å¾Œã«ã€ã‚¯ã‚¨ãƒªã¨ãªã‚‹ç”»åƒã‚’è¿½åŠ ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹æˆ\n",
    "- ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰ã®è©•ä¾¡ã§ã¯ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã‚’ä½¿ç”¨\n",
    "- ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ã‚¨ãƒ³ãƒ‰ã®è©•ä¾¡ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å¯¾æ•°å°¤åº¦ã‚’ç”¨ã„ã¦é¸æŠè‚¢ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7efd0f",
   "metadata": {},
   "source": [
    "## å®Ÿé¨“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250129f",
   "metadata": {},
   "source": [
    "11ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ€§èƒ½ã‚’æ¤œè¨¼ã—ã€ãã®ã†ã¡7ã¤ãŒç”»åƒã¨å‹•ç”»èªè­˜ã‚¿ã‚¹ã‚¯ã§ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šå„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ãŸ:\n",
    "\n",
    "![](image/table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb803e85",
   "metadata": {},
   "source": [
    "FlamingoãŒSotAã«é”ã—ãªã‹ã£ãŸ13ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€å…¥åŠ›è§£åƒåº¦ã‚’ä¸Šã’ã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã™ã‚‹ã¨5ã¤ãŒæ”¹å–„:\n",
    "\n",
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c2c23",
   "metadata": {},
   "source": [
    "Flamingo-3Bã§ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã‚’ã—ãŸã¨ã“ã‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ··åˆï¼ˆM3Wï¼‰ãŒç‰¹ã«é‡è¦ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸ:\n",
    "\n",
    "![](image/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95415762",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6db9e1",
   "metadata": {},
   "source": [
    "[Open-Flamingo][1]ã‚’å‚è€ƒã«ã—ã¦å®Ÿè£…ã™ã‚‹ã€‚\n",
    "\n",
    "[1]: https://github.com/mlfoundations/open_flamingo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91da6c",
   "metadata": {},
   "source": [
    "### ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e38c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.info(\"ãƒ­ã‚°ã‚’åˆæœŸåŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a84523",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q open_clip_torch einops einops_exts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f213689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from einops import repeat\n",
    "from einops_exts import rearrange_many\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "from torch import einsum, nn\n",
    "from torch import nn\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import enable_wrap, wrap\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from typing import Optional\n",
    "import open_clip\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a1d0a",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1897161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    \"\"\"\n",
    "    å€¤ãŒNoneã§ãªã„ã‹ã‚’æ¤œè¨¼ã™ã‚‹\n",
    "\n",
    "    å¼•æ•°:\n",
    "        val: æ¤œè¨¼ã™ã‚‹å€¤\n",
    "    æˆ»ã‚Šå€¤:\n",
    "        bool: å€¤ãŒNoneã§ãªã„å ´åˆã«Trueã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    return val is not None\n",
    "\n",
    "exists(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c389d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getattr_recursive(obj, att):\n",
    "    \"\"\"\n",
    "    ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒã‚¹ãƒˆã•ã‚ŒãŸå±æ€§ã‚’å±æ€§åã§å–å¾—ã™ã‚‹\n",
    "\n",
    "    å¼•æ•°:\n",
    "        obj: å±æ€§ã‚’å–å¾—ã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        att (str): ãƒã‚¹ãƒˆã•ã‚ŒãŸå±æ€§åï¼ˆãƒ‰ãƒƒãƒˆã§åŒºåˆ‡ã‚‰ã‚ŒãŸæ–‡å­—åˆ—ï¼‰\n",
    "    æˆ»ã‚Šå€¤:\n",
    "        å±æ€§ã®å€¤\n",
    "    \"\"\"\n",
    "    if att == \"\":\n",
    "        return obj\n",
    "    i = att.find(\".\")\n",
    "    if i < 0:\n",
    "        return getattr(obj, att)\n",
    "    else:\n",
    "        return getattr_recursive(getattr(obj, att[:i]), att[i + 1 :])\n",
    "\n",
    "# æ¤œè¨¼\n",
    "\n",
    "class Nested:\n",
    "    b = None\n",
    "\n",
    "class Sample:\n",
    "    a = Nested()\n",
    "\n",
    "sample = Sample()\n",
    "sample.a.b = 42\n",
    "getattr_recursive(sample, \"a.b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ce2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setattr_recursive(obj, att, val):\n",
    "    \"\"\"\n",
    "    ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãƒã‚¹ãƒˆã•ã‚ŒãŸå±æ€§ã‚’å±æ€§åã§è¨­å®šã™ã‚‹\n",
    "\n",
    "    å¼•æ•°:\n",
    "        obj: å±æ€§ã‚’è¨­å®šã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        att (str): ãƒã‚¹ãƒˆã•ã‚ŒãŸå±æ€§åï¼ˆãƒ‰ãƒƒãƒˆã§åŒºåˆ‡ã‚‰ã‚ŒãŸæ–‡å­—åˆ—ï¼‰\n",
    "        val: è¨­å®šã™ã‚‹å€¤\n",
    "    \"\"\"\n",
    "    if \".\" in att:\n",
    "        obj = getattr_recursive(obj, \".\".join(att.split(\".\")[:-1]))\n",
    "    setattr(obj, att.split(\".\")[-1], val)\n",
    "\n",
    "# æ¤œè¨¼\n",
    "\n",
    "sample = Sample()\n",
    "setattr_recursive(sample, \"a.b\", 42)\n",
    "sample.a.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bea638",
   "metadata": {},
   "source": [
    "### FeedFoward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(dim, mult=4):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆã™ã‚‹\n",
    "    PerceiverResamplerã¨GatedCrossAttentionã§ä½¿ç”¨ã™ã‚‹\n",
    "\n",
    "    å¼•æ•°:\n",
    "        dim (int): å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã®æ¬¡å…ƒæ•°\n",
    "        mult (int, optional): å†…éƒ¨å±¤ã®æ¬¡å…ƒæ•°ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã®ä¹—æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4ã€‚\n",
    "    æˆ»ã‚Šå€¤:\n",
    "        nn.Sequential: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "    inner_dim = int(dim * mult)\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim, inner_dim, bias=False),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(inner_dim, dim, bias=False),\n",
    "    )\n",
    "\n",
    "ff = FeedForward(128, mult=2)\n",
    "ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cf3d15",
   "metadata": {},
   "source": [
    "### MaskedCrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f98b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒã‚¹ã‚¯ä»˜ãã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    GatedCrossAttentionBlockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_visual,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        only_attend_immediate_media=True,\n",
    "    ):\n",
    "        logger.info(f\"MaskedCrossAttentionã‚’åˆæœŸåŒ– {dim=}, {dim_visual=}, {dim_head=}, {heads=}, {only_attend_immediate_media=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = dim_head**-0.5\n",
    "        logger.info(f\"{self.scale=}\")\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "        logger.info(f\"{inner_dim=}\")\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_kv = nn.Linear(dim_visual, inner_dim * 2, bias=False)\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "        # whether for text to only attend to immediate preceding image, or all previous images\n",
    "        self.only_attend_immediate_media = only_attend_immediate_media\n",
    "\n",
    "        logger.info(\"MaskedCrossAttentionã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "\n",
    "    def forward(self, x, media, media_locations=None, use_cached_media=False):\n",
    "        \"\"\"\n",
    "        ãƒã‚¹ã‚¯ä»˜ãã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é †ä¼æ’­\n",
    "\n",
    "        å¼•æ•°:\n",
    "            x (torch.Tensor): ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡\n",
    "                shape (B, T_txt, D_txt)\n",
    "            media (torch.Tensor): ç”»åƒç‰¹å¾´é‡\n",
    "                shape (B, T_img, n, D_img) ã“ã“ã§nã¯æ½œåœ¨å¤‰æ•°ã®æ¬¡å…ƒ\n",
    "            media_locations: xå†…ã®ãƒ¡ãƒ‡ã‚£ã‚¢ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è­˜åˆ¥ã™ã‚‹ãƒ–ãƒ¼ãƒ«ãƒã‚¹ã‚¯\n",
    "                shape (B, T_txt)\n",
    "            use_cached_media: bool\n",
    "                Trueã®å ´åˆã€media_locationsã§ç™»éŒ²ã•ã‚ŒãŸæœ€å¾Œã®ãƒ¡ãƒ‡ã‚£ã‚¢ä»¥é™ã®ã™ã¹ã¦ã®xã‚’æ‰±ã†ã€‚T_txtã¯media_locations.shape[1]ã¨æ­£ç¢ºã«ç­‰ã—ãã‚ã‚‹å¿…è¦ã¯ãªã„\n",
    "        æˆ»ã‚Šå€¤:\n",
    "            torch.Tensor: ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å‡ºåŠ›\n",
    "        \"\"\"\n",
    "        logger.info(f\"MaskedCrossAttentionã®é †ä¼æ’­é–‹å§‹ {x.shape=}, {media.shape=}, {None if media_locations is None else media_locations.shape=}, {use_cached_media=}\")\n",
    "\n",
    "        if not use_cached_media:\n",
    "            assert (\n",
    "                media_locations.shape[1] == x.shape[1]\n",
    "            ), f\"media_location.shape is {media_locations.shape} but x.shape is {x.shape}\"\n",
    "\n",
    "        T_txt = x.shape[1]\n",
    "\n",
    "        _, T_img, n = media.shape[:3]\n",
    "\n",
    "        h = self.heads\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        q = self.to_q(x)\n",
    "\n",
    "        media = rearrange(media, \"b t n d -> b (t n) d\")\n",
    "\n",
    "        k, v = self.to_kv(media).chunk(2, dim=-1)\n",
    "        q, k, v = rearrange_many((q, k, v), \"b n (h d) -> b h n d\", h=h)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"... i d, ... j d -> ... i j\", q, k)\n",
    "\n",
    "        if exists(media_locations):\n",
    "            media_time = torch.arange(T_img, device=x.device) + 1\n",
    "\n",
    "            if use_cached_media:\n",
    "                # text time is set to the last cached media location\n",
    "                text_time = repeat(\n",
    "                    torch.count_nonzero(media_locations, dim=1),\n",
    "                    \"b -> b i\",\n",
    "                    i=T_txt,\n",
    "                )\n",
    "            else:\n",
    "                # at each boolean of True, increment the time counter (relative to media time)\n",
    "                text_time = media_locations.cumsum(dim=-1)\n",
    "\n",
    "            # text time must equal media time if only attending to most immediate image\n",
    "            # otherwise, as long as text time is greater than media time (if attending to all previous images / media)\n",
    "            mask_op = torch.eq if self.only_attend_immediate_media else torch.ge\n",
    "\n",
    "            text_to_media_mask = mask_op(\n",
    "                rearrange(text_time, \"b i -> b 1 i 1\"),\n",
    "                repeat(media_time, \"j -> 1 1 1 (j n)\", n=n),\n",
    "            )\n",
    "            sim = sim.masked_fill(~text_to_media_mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        if exists(media_locations) and self.only_attend_immediate_media:\n",
    "            # any text without a preceding media needs to have attention zeroed out\n",
    "            text_without_media_mask = text_time == 0\n",
    "            text_without_media_mask = rearrange(\n",
    "                text_without_media_mask, \"b i -> b 1 i 1\"\n",
    "            )\n",
    "            attn = attn.masked_fill(text_without_media_mask, 0.0)\n",
    "\n",
    "        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        logger.info(f\"MaskedCrossAttentionã®é †ä¼æ’­å®Œäº† {out.shape=}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85662b6e",
   "metadata": {},
   "source": [
    "### GatedCrossAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedCrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ã‚²ãƒ¼ãƒˆä»˜ãã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    FlamingoLayerã¨FlamingoLMMixinã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_visual,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        ff_mult=4,\n",
    "        only_attend_immediate_media=True,\n",
    "    ):\n",
    "        logger.info(f\"GatedCrossAttentionBlockã‚’åˆæœŸåŒ– {dim=}, {dim_visual=}, {dim_head=}, {heads=}, {ff_mult=}, {only_attend_immediate_media=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MaskedCrossAttention(\n",
    "            dim=dim,\n",
    "            dim_visual=dim_visual,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            only_attend_immediate_media=only_attend_immediate_media,\n",
    "        )\n",
    "        self.attn_gate = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        self.ff = FeedForward(dim, mult=ff_mult)\n",
    "        self.ff_gate = nn.Parameter(torch.tensor([0.0]))\n",
    "        logger.info(\"GatedCrossAttentionBlockã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        media,\n",
    "        media_locations=None,\n",
    "        use_cached_media=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ã‚²ãƒ¼ãƒˆä»˜ãã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã®é †ä¼æ’­\n",
    "\n",
    "        å¼•æ•°:\n",
    "            x (torch.Tensor): ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡\n",
    "                shape (B, T_txt, D_txt)\n",
    "            media (torch.Tensor): ç”»åƒç‰¹å¾´é‡\n",
    "                shape (B, T_img, n, D_img) ã“ã“ã§nã¯æ½œåœ¨å¤‰æ•°ã®æ¬¡å…ƒ\n",
    "            media_locations: xå†…ã®ãƒ¡ãƒ‡ã‚£ã‚¢ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è­˜åˆ¥ã™ã‚‹ãƒ–ãƒ¼ãƒ«ãƒã‚¹ã‚¯\n",
    "                shape (B, T_txt)\n",
    "            use_cached_media: bool\n",
    "                Trueã®å ´åˆã€media_locationsã§ç™»éŒ²ã•ã‚ŒãŸæœ€å¾Œã®ãƒ¡ãƒ‡ã‚£ã‚¢ä»¥é™ã®ã™ã¹ã¦ã®xã‚’æ‰±ã†ã€‚T_txtã¯media_locations.shape[1]ã¨æ­£ç¢ºã«ç­‰ã—ãã‚ã‚‹å¿…è¦ã¯ãªã„\n",
    "\n",
    "        æˆ»ã‚Šå€¤:\n",
    "            torch.Tensor: ã‚²ãƒ¼ãƒˆä»˜ãã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›\n",
    "        \"\"\"\n",
    "        logger.info(f\"GatedCrossAttentionBlockã®é †ä¼æ’­é–‹å§‹ {x.shape=}, {x.dtype=} {media.shape=}, {None if media_locations is None else media_locations.shape=}, {use_cached_media=}\")\n",
    "    \n",
    "        x = (\n",
    "            self.attn(\n",
    "                x,\n",
    "                media,\n",
    "                media_locations=media_locations,\n",
    "                use_cached_media=use_cached_media,\n",
    "            )\n",
    "            * self.attn_gate.tanh()\n",
    "            + x\n",
    "        )\n",
    "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
    "\n",
    "        logger.info(f\"GatedCrossAttentionBlockã®é †ä¼æ’­å®Œäº† {x.shape=} {x.dtype=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5bc540",
   "metadata": {},
   "source": [
    "### PerceiverAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceiver Attentionãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    PerceiverResamplerã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, dim, dim_head=64, heads=8):\n",
    "        logger.info(f\"PerceiverAttentionã‚’åˆæœŸåŒ– {dim=}, {dim_head=}, {heads=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm_media = nn.LayerNorm(dim)\n",
    "        self.norm_latents = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "        logger.info(\"PerceiverAttentionã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x, latents):\n",
    "        \"\"\"\n",
    "        Perceiver Attentionã®é †ä¼æ’­\n",
    "\n",
    "        å¼•æ•°:\n",
    "            x (torch.Tensor): ç”»åƒç‰¹å¾´é‡\n",
    "                shape (b, T, n1, D)\n",
    "            latents (torch.Tensor): æ½œåœ¨ç‰¹å¾´é‡\n",
    "                shape (b, T, n2, D)\n",
    "                shape (b, T, n2, D)\n",
    "\n",
    "        æˆ»ã‚Šå€¤:\n",
    "            torch.Tensor: Perceiver Attentionã®å‡ºåŠ›\n",
    "        \"\"\"\n",
    "        logger.info(f\"PerceiverAttentionã®é †ä¼æ’­é–‹å§‹ {x.shape=}, {x.dtype=} {latents.shape=}, {latents.dtype=}\")\n",
    "        x = self.norm_media(x)\n",
    "        latents = self.norm_latents(latents)\n",
    "\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(latents)\n",
    "        kv_input = torch.cat((x, latents), dim=-2)\n",
    "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
    "        q, k, v = rearrange_many((q, k, v), \"b t n (h d) -> b h t n d\", h=h)\n",
    "        q = q * self.scale\n",
    "\n",
    "        # attention\n",
    "        sim = einsum(\"... i d, ... j d  -> ... i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n",
    "        out = rearrange(out, \"b h t n d -> b t n (h d)\", h=h)\n",
    "        out = self.to_out(out)\n",
    "        logger.info(f\"PerceiverAttentionã®é †ä¼æ’­å®Œäº† {out.shape=} {out.dtype=}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db2e79",
   "metadata": {},
   "source": [
    "### PerceiverResampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceiver Resamplerãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    Flamingoãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth=6,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        num_latents=64,\n",
    "        max_num_media=None,\n",
    "        max_num_frames=None,\n",
    "        ff_mult=4,\n",
    "    ):\n",
    "        logger.info(f\"PerceiverResamplerã‚’åˆæœŸåŒ– {dim=}, {depth=}, {dim_head=}, {heads=}, {num_latents=}, {max_num_media=}, {max_num_frames=}, {ff_mult=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "        self.frame_embs = (\n",
    "            nn.Parameter(torch.randn(max_num_frames, dim))\n",
    "            if exists(max_num_frames)\n",
    "            else None\n",
    "        )\n",
    "        self.media_time_embs = (\n",
    "            nn.Parameter(torch.randn(max_num_media, 1, dim))\n",
    "            if exists(max_num_media)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
    "                        FeedForward(dim=dim, mult=ff_mult),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        logger.info(\"PerceiverResamplerã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perceiver Resamplerã®é †ä¼æ’­\n",
    "\n",
    "        å¼•æ•°:\n",
    "            x (torch.Tensor): ç”»åƒç‰¹å¾´é‡\n",
    "                shape (b, T, F, v, D)\n",
    "        æˆ»ã‚Šå€¤:\n",
    "            torch.Tensor: Perceiver Resamplerã®å‡ºåŠ›\n",
    "                shape (b, T, n, D) where n is self.num_latents\n",
    "        \"\"\"\n",
    "        logger.info(f\"PerceiverResamplerã®é †ä¼æ’­é–‹å§‹ {x.shape=}, {x.dtype=}\")\n",
    "\n",
    "        b, T, F, v = x.shape[:4]\n",
    "\n",
    "        # frame and media time embeddings\n",
    "        if exists(self.frame_embs):\n",
    "            frame_embs = repeat(self.frame_embs[:F], \"F d -> b T F v d\", b=b, T=T, v=v)\n",
    "            x = x + frame_embs\n",
    "        x = rearrange(\n",
    "            x, \"b T F v d -> b T (F v) d\"\n",
    "        )  # flatten the frame and spatial dimensions\n",
    "        if exists(self.media_time_embs):\n",
    "            x = x + self.media_time_embs[:T]\n",
    "\n",
    "        # blocks\n",
    "        latents = repeat(self.latents, \"n d -> b T n d\", b=b, T=T)\n",
    "        for attn, ff in self.layers:\n",
    "            latents = attn(x, latents) + latents\n",
    "            latents = ff(latents) + latents\n",
    "\n",
    "        out = self.norm(latents)\n",
    "        logger.info(f\"PerceiverResamplerã®é †ä¼æ’­å®Œäº† {out.shape=} {out.dtype=}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20586baa",
   "metadata": {},
   "source": [
    "### Flamingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flamingo(nn.Module):\n",
    "    \"\"\"\n",
    "    Flamingoãƒ¢ãƒ‡ãƒ«\n",
    "    ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨è¨€èªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_encoder: nn.Module,\n",
    "        lang_encoder: nn.Module,\n",
    "        eoc_token_id: int,\n",
    "        media_token_id: int,\n",
    "        vis_dim: int,\n",
    "        cross_attn_every_n_layers: int = 1,\n",
    "        gradient_checkpointing: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Flamingoãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
    "\n",
    "        å¼•æ•°:\n",
    "            vision_encoder (nn.Module): Hugging Faceã®CLIPModel\n",
    "            lang_encoder (nn.Module): Hugging Faceã®å› æœè¨€èªãƒ¢ãƒ‡ãƒ«\n",
    "            eoc_token_id (int): <|endofchunk|>ã®ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "            media_token_id (int): <image>ã®ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "            vis_dim (int): ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°\n",
    "                ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ç‰¹å¾´é‡ã¯ã“ã®å½¢çŠ¶ã«åˆã‚ã›ã¦æœ€å¾Œã®æ¬¡å…ƒã§æŠ•å½±ã•ã‚Œã‚‹\n",
    "            cross_attn_every_n_layers (int, optional): ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å¾Œã«ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã™ã‚‹é »åº¦ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã€‚\n",
    "            gradient_checkpointing (bool, optional): å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã€‚\n",
    "        æˆ»ã‚Šå€¤:\n",
    "            None\n",
    "        \"\"\"\n",
    "        logger.info(f\"Flamingoãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ– {eoc_token_id=}, {media_token_id=}, {vis_dim=}, {cross_attn_every_n_layers=}, {gradient_checkpointing=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.eoc_token_id = eoc_token_id\n",
    "        self.media_token_id = media_token_id\n",
    "        self.vis_dim = vis_dim\n",
    "        if hasattr(lang_encoder.config, \"d_model\"):\n",
    "            self.lang_dim = lang_encoder.config.d_model  # mpt uses d_model\n",
    "        else:\n",
    "            self.lang_dim = lang_encoder.config.hidden_size\n",
    "\n",
    "        self.vision_encoder = vision_encoder.visual\n",
    "        self.perceiver = PerceiverResampler(dim=self.vis_dim)\n",
    "        self.lang_encoder = lang_encoder\n",
    "        self.lang_encoder.init_flamingo(\n",
    "            media_token_id=media_token_id,\n",
    "            lang_hidden_size=self.lang_dim,\n",
    "            vis_hidden_size=self.vis_dim,\n",
    "            cross_attn_every_n_layers=cross_attn_every_n_layers,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "        )\n",
    "        self._use_gradient_checkpointing = gradient_checkpointing\n",
    "        self.perceiver._use_gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "        logger.info(\"Flamingoãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vision_x: torch.Tensor,\n",
    "        lang_x: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None,\n",
    "        clear_conditioned_layers: bool = True,\n",
    "        past_key_values=None,\n",
    "        use_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Flamingoã®é †ä¼æ’­\n",
    "\n",
    "        å¼•æ•°:\n",
    "            vision_x (torch.Tensor): ãƒ“ã‚¸ãƒ§ãƒ³å…¥åŠ›\n",
    "                shape (B, T_img, F, C, H, W) with F=1\n",
    "            lang_x (torch.Tensor): è¨€èªå…¥åŠ›ID\n",
    "                shape (B, T_txt)\n",
    "            attention_mask (torch.Tensor, optional): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            labels (torch.Tensor, optional): ãƒ©ãƒ™ãƒ«ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            clear_conditioned_layers: Trueã®å ´åˆã€é †ä¼æ’­ãŒå®Œäº†ã—ãŸã‚‰æ¡ä»¶ä»˜ããƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚\n",
    "                åŒã˜ã‚»ãƒƒãƒˆã®ç”»åƒãŒåˆ¥ã®é †ä¼æ’­ã§å†åˆ©ç”¨ã•ã‚Œã‚‹å ´åˆã¯ã€ã“ã‚Œã‚’falseã«è¨­å®šã—ã¾ã™ã€‚\n",
    "            past_key_values: äº‹å‰è¨ˆç®—ã•ã‚ŒãŸå€¤ã‚’è¨€èªãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã¾ã™ã€‚\n",
    "                Hugging Faceã®CausalLMãƒ¢ãƒ‡ãƒ«ã®past_key_valuesãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "            use_cache: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚­ãƒ¼å€¤ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã€‚Hugging Face CausalLMãƒ¢ãƒ‡ãƒ«ã®use_cacheãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
    "        æˆ»ã‚Šå€¤:\n",
    "            CausalLMOutputWithPast: è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›\n",
    "        \"\"\"\n",
    "        logger.info(f\"Flamingoã®é †ä¼æ’­é–‹å§‹ {vision_x.shape=}, {lang_x.shape=}, {None if attention_mask is None else attention_mask.shape=}, {None if labels is None else labels.shape=}, {clear_conditioned_layers=}, {use_cache=}\")\n",
    "\n",
    "        assert (\n",
    "            self.lang_encoder.initialized_flamingo\n",
    "        ), \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n",
    "\n",
    "        assert (\n",
    "            self.lang_encoder._use_cached_vision_x or vision_x is not None\n",
    "        ), \"Must provide either vision_x or have precached media using cache_media().\"\n",
    "\n",
    "        if self.lang_encoder._use_cached_vision_x:\n",
    "            # Case: use cached; vision_x should be cached and other\n",
    "            # vision-related inputs should not be provided.\n",
    "            assert (\n",
    "                vision_x is None\n",
    "            ), \"Expect vision_x to be None when media has been cached using cache_media(). Try uncache_media() first.\"\n",
    "            assert self.lang_encoder.is_conditioned()\n",
    "\n",
    "        else:\n",
    "            # Case: do not use caching (i.e. this is a standard forward pass);\n",
    "            self._encode_vision_x(vision_x=vision_x)\n",
    "            self._condition_media_locations(input_ids=lang_x)\n",
    "\n",
    "        output = self.lang_encoder(\n",
    "            input_ids=lang_x,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        if clear_conditioned_layers:\n",
    "            self.lang_encoder.clear_conditioned_layers()\n",
    "\n",
    "        logger.info(f\"Flamingoã®é †ä¼æ’­å®Œäº†\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        vision_x: torch.Tensor,\n",
    "        lang_x: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Flamingoã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
    "\n",
    "        å¼•æ•°:\n",
    "            vision_x (torch.Tensor): ãƒ“ã‚¸ãƒ§ãƒ³å…¥åŠ›\n",
    "                shape (B, T_img, F, C, H, W)\n",
    "                åŒã˜ãƒãƒ£ãƒ³ã‚¯å†…ã®ç”»åƒã¯T_imgã«æ²¿ã£ã¦é€£çµã•ã‚Œã€ãƒ•ãƒ¬ãƒ¼ãƒ ã¯Fã«æ²¿ã£ã¦é€£çµã•ã‚Œã¾ã™\n",
    "                ç¾åœ¨ã€F=1ã®ã¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ï¼ˆå˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ“ãƒ‡ã‚ªï¼‰\n",
    "            lang_x (torch.Tensor): è¨€èªå…¥åŠ›\n",
    "                shape (B, T_txt)\n",
    "            **kwargs: Hugging Face CausalLMãƒ¢ãƒ‡ãƒ«ã®generateãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚æ³¨ç›®ã™ã¹ãkwargs:\n",
    "                max_length (int, optional): å‡ºåŠ›ã®æœ€å¤§é•·ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "                attention_mask (torch.Tensor, optional): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "                num_beams (int, optional): ãƒ“ãƒ¼ãƒ ã®æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã€‚\n",
    "                max_new_tokens (int, optional): æœ€å¤§æ–°è¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "                temperature (float, optional): æ¸©åº¦ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã€‚\n",
    "                top_k (int, optional): ãƒˆãƒƒãƒ—kã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯50ã€‚\n",
    "                top_p (float, optional): ãƒˆãƒƒãƒ—pã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã€‚\n",
    "                no_repeat_ngram_size (int, optional): ç¹°ã‚Šè¿”ã—ç¦æ­¢ngramã‚µã‚¤ã‚ºã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã€‚\n",
    "                length_penalty (float, optional): é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã€‚\n",
    "                num_return_sequences (int, optional): è¿”ã•ã‚Œã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã€‚\n",
    "                do_sample (bool, optional): ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã‹ã©ã†ã‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã€‚\n",
    "                early_stopping (bool, optional): æ—©æœŸåœæ­¢ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã€‚\n",
    "        æˆ»ã‚Šå€¤:\n",
    "            torch.Tensor: ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ãŒè¿½åŠ ã•ã‚ŒãŸlang_x\n",
    "        \"\"\"\n",
    "        logger.info(f\"Flamingoã®generateé–‹å§‹ {vision_x.shape=}, {lang_x.shape=}, {None if attention_mask is None else attention_mask.shape=}, {kwargs.keys()}\")\n",
    "        num_beams = kwargs.pop(\"num_beams\", 1)\n",
    "        if num_beams > 1:\n",
    "            vision_x = vision_x.repeat_interleave(num_beams, dim=0)\n",
    "\n",
    "        self.lang_encoder._use_cached_vision_x = True\n",
    "        self._encode_vision_x(vision_x=vision_x)\n",
    "\n",
    "        eos_token_id = kwargs.pop(\"eos_token_id\", self.eoc_token_id)\n",
    "        output = self.lang_encoder.generate(\n",
    "            input_ids=lang_x,\n",
    "            attention_mask=attention_mask,\n",
    "            eos_token_id=eos_token_id,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.lang_encoder.clear_conditioned_layers()\n",
    "        self.lang_encoder._use_cached_vision_x = False\n",
    "        logger.info(f\"Flamingoã®generateå®Œäº† {output.shape=} {output.dtype=}\")\n",
    "        return output\n",
    "\n",
    "    def _encode_vision_x(self, vision_x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute media tokens from vision input by passing it through vision encoder and conditioning language model.\n",
    "        Args:\n",
    "            vision_x (torch.Tensor): Vision input\n",
    "                shape (B, T_img, F, C, H, W)\n",
    "                Images in the same chunk are collated along T_img, and frames are collated along F\n",
    "                Currently only F=1 is supported (single-frame videos)\n",
    "\n",
    "        rearrange code based on https://github.com/dhansmair/flamingo-mini\n",
    "        \"\"\"\n",
    "\n",
    "        assert vision_x.ndim == 6, \"vision_x should be of shape (b, T_img, F, C, H, W)\"\n",
    "        b, T, F = vision_x.shape[:3]\n",
    "        assert F == 1, \"Only single frame supported\"\n",
    "\n",
    "        vision_x = rearrange(vision_x, \"b T F c h w -> (b T F) c h w\")\n",
    "        with torch.no_grad():\n",
    "            vision_x = self.vision_encoder(vision_x)[1]\n",
    "        vision_x = rearrange(vision_x, \"(b T F) v d -> b T F v d\", b=b, T=T, F=F)\n",
    "        vision_x = self.perceiver(vision_x)\n",
    "\n",
    "        for layer in self.lang_encoder._get_decoder_layers():\n",
    "            layer.condition_vis_x(vision_x)\n",
    "\n",
    "    def wrap_fsdp(self, wrapper_kwargs, device_id):\n",
    "        \"\"\"\n",
    "        Manually wraps submodules for FSDP and move other parameters to device_id.\n",
    "\n",
    "        Why manually wrap?\n",
    "        - all parameters within the FSDP wrapper must have the same requires_grad.\n",
    "            We have a mix of frozen and unfrozen parameters.\n",
    "        - model.vision_encoder.visual needs to be individually wrapped or encode_vision_x errors\n",
    "            See: https://github.com/pytorch/pytorch/issues/82461#issuecomment-1269136344\n",
    "\n",
    "        The rough wrapping structure is:\n",
    "        - FlamingoModel\n",
    "            - FSDP(FSDP(vision_encoder))\n",
    "            - FSDP(FSDP(perceiver))\n",
    "            - lang_encoder\n",
    "                - FSDP(FSDP(input_embeddings))\n",
    "                - FlamingoLayers\n",
    "                    - FSDP(FSDP(gated_cross_attn_layer))\n",
    "                    - FSDP(FSDP(decoder_layer))\n",
    "                - FSDP(FSDP(output_embeddings))\n",
    "                - other parameters\n",
    "\n",
    "        Known issues:\n",
    "        - Our FSDP strategy is not compatible with tied embeddings. If the LM embeddings are tied,\n",
    "            train with DDP or set the --freeze_lm_embeddings flag to true.\n",
    "        - With FSDP + gradient ckpting, one can increase the batch size with seemingly no upper bound.\n",
    "            Although the training curves look okay, we found that downstream performance dramatically\n",
    "            degrades if the batch size is unreasonably large (e.g., 100 MMC4 batch size for OPT-125M).\n",
    "\n",
    "        FAQs about our FSDP wrapping strategy:\n",
    "        Why double wrap?\n",
    "        As of torch==2.0.1, FSDP's _post_forward_hook and _post_backward_hook\n",
    "        only free gathered parameters if the module is NOT FSDP root.\n",
    "\n",
    "        Why unfreeze the decoder_layers?\n",
    "        See https://github.com/pytorch/pytorch/issues/95805\n",
    "        As of torch==2.0.1, FSDP's _post_backward_hook is only registed if the flat param\n",
    "        requires_grad=True. We need the postback to fire to avoid OOM.\n",
    "        To effectively freeze the decoder layers, we exclude them from the optimizer.\n",
    "\n",
    "        What is assumed to be frozen v. unfrozen?\n",
    "        We assume that the model is being trained under normal Flamingo settings\n",
    "        with these lines being called in factory.py:\n",
    "            ```\n",
    "            # Freeze all parameters\n",
    "            model.requires_grad_(False)\n",
    "            assert sum(p.numel() for p in model.parameters() if p.requires_grad) == 0\n",
    "\n",
    "            # Unfreeze perceiver, gated_cross_attn_layers, and LM input embeddings\n",
    "            model.perceiver.requires_grad_(True)\n",
    "            model.lang_encoder.gated_cross_attn_layers.requires_grad_(True)\n",
    "            [optional] model.lang_encoder.get_input_embeddings().requires_grad_(True)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        # unfreeze the decoder layers\n",
    "        for block in self.lang_encoder.old_decoder_blocks:\n",
    "            block.requires_grad_(True)\n",
    "\n",
    "        # wrap in FSDP\n",
    "        with enable_wrap(wrapper_cls=FSDP, **wrapper_kwargs):\n",
    "            self.perceiver = wrap(wrap(self.perceiver))\n",
    "            self.lang_encoder.old_decoder_blocks = nn.ModuleList(\n",
    "                wrap(wrap(block)) for block in self.lang_encoder.old_decoder_blocks\n",
    "            )\n",
    "            self.lang_encoder.gated_cross_attn_layers = nn.ModuleList(\n",
    "                wrap(wrap(layer)) if layer is not None else None\n",
    "                for layer in self.lang_encoder.gated_cross_attn_layers\n",
    "            )\n",
    "            self.lang_encoder.init_flamingo_layers(self._use_gradient_checkpointing)\n",
    "            self.lang_encoder.set_input_embeddings(\n",
    "                wrap(wrap(self.lang_encoder.get_input_embeddings()))\n",
    "            )\n",
    "            self.lang_encoder.set_output_embeddings(\n",
    "                wrap(wrap(self.lang_encoder.get_output_embeddings()))\n",
    "            )\n",
    "            self.vision_encoder = wrap(wrap(self.vision_encoder))  # frozen\n",
    "\n",
    "        # manually move non-FSDP managed parameters to device_id\n",
    "        # these are all in lang_encoder\n",
    "        apply_with_stopping_condition(\n",
    "            module=self.lang_encoder,\n",
    "            apply_fn=lambda m: m.to(device_id),\n",
    "            apply_condition=lambda m: len(list(m.children())) == 0,\n",
    "            stopping_condition=lambda m: isinstance(m, FSDP),\n",
    "        )\n",
    "\n",
    "        # exclude the original decoder layers from the optimizer\n",
    "        for block in self.lang_encoder.old_decoder_blocks:\n",
    "            for p in block.parameters():\n",
    "                p.exclude_from_optimizer = True\n",
    "\n",
    "        # set up clip_grad_norm_ function\n",
    "        def clip_grad_norm_(max_norm):\n",
    "            self.perceiver.clip_grad_norm_(max_norm)\n",
    "            for layer in self.lang_encoder.gated_cross_attn_layers:\n",
    "                if layer is not None:\n",
    "                    layer.clip_grad_norm_(max_norm)\n",
    "            self.lang_encoder.get_input_embeddings().clip_grad_norm_(max_norm)\n",
    "\n",
    "        self.clip_grad_norm_ = clip_grad_norm_\n",
    "\n",
    "    def _condition_media_locations(self, input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the media token locations from lang_x and condition the language model on these.\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Language input\n",
    "                shape (B, T_txt)\n",
    "        \"\"\"\n",
    "        media_locations = input_ids == self.media_token_id\n",
    "\n",
    "        for layer in self.lang_encoder._get_decoder_layers():\n",
    "            layer.condition_media_locations(media_locations)\n",
    "\n",
    "    def cache_media(self, input_ids: torch.Tensor, vision_x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Pre-cache a prompt/sequence of images / text for log-likelihood evaluations.\n",
    "        All subsequent calls to forward() will generate attending to the LAST\n",
    "        image in vision_x.\n",
    "        This is not meant to be used to cache things for generate().\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Language input\n",
    "                shape (B, T_txt)\n",
    "            vision_x (torch.Tensor): Vision input\n",
    "                shape (B, T_img, F, C, H, W)\n",
    "                Images in the same chunk are collated along T_img, and frames are collated along F\n",
    "                Currently only F=1 is supported (single-frame videos)\n",
    "        \"\"\"\n",
    "        self._encode_vision_x(vision_x=vision_x)\n",
    "        self._condition_media_locations(input_ids=input_ids)\n",
    "        self.lang_encoder._use_cached_vision_x = True\n",
    "\n",
    "    def uncache_media(self):\n",
    "        \"\"\"\n",
    "        Clear all conditioning.\n",
    "        \"\"\"\n",
    "        self.lang_encoder.clear_conditioned_layers()\n",
    "        self.lang_encoder._use_cached_vision_x = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e2c9b",
   "metadata": {},
   "source": [
    "### FlamingoLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlamingoLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Flamingoãƒ¬ã‚¤ãƒ¤ãƒ¼\n",
    "    GatedCrossAttentionBlockã¨DecoderLayerã®ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "    FlamingoMixinã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, gated_cross_attn_layer, decoder_layer, gradient_checkpointing=False\n",
    "    ):\n",
    "        logger.info(f\"FlamingoLayerã‚’åˆæœŸåŒ– {gradient_checkpointing=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.gated_cross_attn_layer = gated_cross_attn_layer\n",
    "        self.decoder_layer = decoder_layer\n",
    "        self.vis_x = None\n",
    "        self.media_locations = None\n",
    "        if self.gated_cross_attn_layer is not None:\n",
    "            self.gated_cross_attn_layer._use_gradient_checkpointing = (\n",
    "                gradient_checkpointing\n",
    "            )\n",
    "        self.decoder_layer._use_gradient_checkpointing = gradient_checkpointing\n",
    "        logger.info(\"FlamingoLayerã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def is_conditioned(self) -> bool:\n",
    "        \"\"\"Check whether the layer is conditioned.\"\"\"\n",
    "        return self.vis_x is not None and self.media_locations is not None\n",
    "\n",
    "    # Used this great idea from this implementation of Flamingo (https://github.com/dhansmair/flamingo-mini/)\n",
    "    def condition_vis_x(self, vis_x):\n",
    "        self.vis_x = vis_x\n",
    "\n",
    "    def condition_media_locations(self, media_locations):\n",
    "        self.media_locations = media_locations\n",
    "\n",
    "    def condition_use_cached_media(self, use_cached_media):\n",
    "        self.use_cached_media = use_cached_media\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        lang_x,\n",
    "        attention_mask=None,\n",
    "        **decoder_layer_kwargs,\n",
    "    ):\n",
    "        # Cross attention\n",
    "        if self.gated_cross_attn_layer is not None:\n",
    "            if self.vis_x is None:\n",
    "                raise ValueError(\"vis_x must be conditioned before forward pass\")\n",
    "\n",
    "            if self.media_locations is None:\n",
    "                raise ValueError(\n",
    "                    \"media_locations must be conditioned before forward pass\"\n",
    "                )\n",
    "\n",
    "            lang_x = self.gated_cross_attn_layer(\n",
    "                lang_x,\n",
    "                self.vis_x,\n",
    "                media_locations=self.media_locations,\n",
    "                use_cached_media=self.use_cached_media,\n",
    "            )\n",
    "\n",
    "        # Normal decoder layer\n",
    "        lang_x = self.decoder_layer(\n",
    "            lang_x, attention_mask=attention_mask, **decoder_layer_kwargs\n",
    "        )\n",
    "        return lang_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e440c5",
   "metadata": {},
   "source": [
    "### FlamingoLMMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf010e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlamingoLMMixin(nn.Module):\n",
    "    \"\"\"\n",
    "    è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã®ãƒŸãƒƒã‚¯ã‚¹ã‚¤ãƒ³\n",
    "    \"\"\"\n",
    "\n",
    "    def set_decoder_layers_attr_name(self, decoder_layers_attr_name):\n",
    "        self.decoder_layers_attr_name = decoder_layers_attr_name\n",
    "\n",
    "    def _get_decoder_layers(self):\n",
    "        return getattr_recursive(self, self.decoder_layers_attr_name)\n",
    "\n",
    "    def _set_decoder_layers(self, value):\n",
    "        setattr_recursive(self, self.decoder_layers_attr_name, value)\n",
    "\n",
    "    def init_flamingo(\n",
    "        self,\n",
    "        media_token_id,\n",
    "        lang_hidden_size,\n",
    "        vis_hidden_size,\n",
    "        cross_attn_every_n_layers,\n",
    "        gradient_checkpointing,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Flamingo by adding a new gated cross attn to the decoder. Store the media token id for computing the media locations.\n",
    "        \"\"\"\n",
    "        self.old_decoder_blocks = self._get_decoder_layers()\n",
    "        self.gated_cross_attn_layers = nn.ModuleList(\n",
    "            [\n",
    "                GatedCrossAttentionBlock(\n",
    "                    dim=lang_hidden_size, dim_visual=vis_hidden_size\n",
    "                )\n",
    "                if (layer_idx + 1) % cross_attn_every_n_layers == 0\n",
    "                else None\n",
    "                for layer_idx, _ in enumerate(self._get_decoder_layers())\n",
    "            ]\n",
    "        )\n",
    "        self.init_flamingo_layers(gradient_checkpointing)\n",
    "        self.media_token_id = media_token_id\n",
    "        self.initialized_flamingo = True\n",
    "        self._use_cached_vision_x = False\n",
    "\n",
    "    def init_flamingo_layers(self, gradient_checkpointing):\n",
    "        \"\"\"\n",
    "        Re initializes the FlamingoLayers.\n",
    "        Propagates any changes made to self.gated_corss_attn_layers or self.old_decoder_blocks\n",
    "        \"\"\"\n",
    "        self._set_decoder_layers(\n",
    "            nn.ModuleList(\n",
    "                [\n",
    "                    FlamingoLayer(\n",
    "                        gated_cross_attn_layer, decoder_layer, gradient_checkpointing\n",
    "                    )\n",
    "                    for gated_cross_attn_layer, decoder_layer in zip(\n",
    "                        self.gated_cross_attn_layers, self.old_decoder_blocks\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        \"\"\"Condition the Flamingo layers on the media locations before forward()\"\"\"\n",
    "        if not self.initialized_flamingo:\n",
    "            raise ValueError(\n",
    "                \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n",
    "            )\n",
    "\n",
    "        media_locations = input_ids == self.media_token_id\n",
    "\n",
    "        # if there are media already cached and we're generating and there are no media tokens in the input,\n",
    "        # we'll assume that ALL input tokens should attend to the last previous media that is cached.\n",
    "        # this is especially important for HF generate() compatibility, since generate() calls forward()\n",
    "        # repeatedly one token at a time (with no media tokens).\n",
    "        # without this check, the model would not attend to any images when generating (after the first token)\n",
    "        use_cached_media_locations = (\n",
    "            self._use_cached_vision_x\n",
    "            and self.is_conditioned()\n",
    "            and not media_locations.any()\n",
    "        )\n",
    "\n",
    "        for layer in self._get_decoder_layers():\n",
    "            if not use_cached_media_locations:\n",
    "                layer.condition_media_locations(media_locations)\n",
    "            layer.condition_use_cached_media(use_cached_media_locations)\n",
    "\n",
    "        # package arguments for the other parent's forward. since we don't know the order of the arguments,\n",
    "        # make them all kwargs\n",
    "        kwargs[\"input_ids\"] = input_ids\n",
    "        kwargs[\"attention_mask\"] = attention_mask\n",
    "        return super().forward(**kwargs)  # Call the other parent's forward method\n",
    "\n",
    "    def is_conditioned(self) -> bool:\n",
    "        \"\"\"Check whether all decoder layers are already conditioned.\"\"\"\n",
    "        return all(l.is_conditioned() for l in self._get_decoder_layers())\n",
    "\n",
    "    def clear_conditioned_layers(self):\n",
    "        for layer in self._get_decoder_layers():\n",
    "            layer.condition_vis_x(None)\n",
    "            layer.condition_media_locations(None)\n",
    "            layer.condition_use_cached_media(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7652e44",
   "metadata": {},
   "source": [
    "### æ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abea430",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_vision_encoder_path=\"ViT-L-14\"\n",
    "clip_vision_encoder_pretrained=\"openai\"\n",
    "lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\"\n",
    "tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\"\n",
    "cross_attn_every_n_layers=1\n",
    "cache_dir=None\n",
    "use_local_files = False\n",
    "decoder_layers_attr_name: str = None\n",
    "freeze_lm_embeddings: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´æ¸ˆã¿ã®CLIPã®è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "vision_encoder, _, image_processor = open_clip.create_model_and_transforms(\n",
    "    clip_vision_encoder_path,\n",
    "    pretrained=clip_vision_encoder_pretrained,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "# è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãŒè¦–è¦šç‰¹å¾´é‡ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«è¨­å®š\n",
    "vision_encoder.visual.output_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    local_files_only=use_local_files,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "# Flamingã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«è¿½åŠ \n",
    "text_tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": [\"<|endofchunk|>\", \"<image>\"]}\n",
    ")\n",
    "\n",
    "if text_tokenizer.pad_token is None:\n",
    "    # GPT2ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒãŸãªã„\n",
    "    # æå¤±ã®ãŸã‚ã«ãƒ©ãƒ™ãƒ«ã‚’ä¿®æ­£ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹\n",
    "    text_tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    logger.info(\"ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \")\n",
    "\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ {text_tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcfe9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´æ¸ˆã¿ã®è¨€èªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "lang_encoder = AutoModelForCausalLM.from_pretrained(\n",
    "    lang_encoder_path,\n",
    "    local_files_only=use_local_files,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’èª­ã¿è¾¼ã¿å®Œäº† {lang_encoder=} {lang_encoder.config=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_instance(obj, mixin):\n",
    "    \"\"\"Apply mixins to a class instance after creation\"\"\"\n",
    "    base_cls = obj.__class__\n",
    "    base_cls_name = obj.__class__.__name__\n",
    "    obj.__class__ = type(\n",
    "        base_cls_name, (mixin, base_cls), {}\n",
    "    )  # mixin needs to go first for our forward() logic to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mpt-1b-redpajama-200b\" in lang_encoder_path:\n",
    "\n",
    "    class EmbeddingFnMixin:\n",
    "        def get_input_embeddings(self):\n",
    "            return self.transformer.wte\n",
    "\n",
    "        def set_input_embeddings(self, new_embeddings):\n",
    "            self.transformer.wte = new_embeddings\n",
    "\n",
    "    extend_instance(lang_encoder, EmbeddingFnMixin)\n",
    "    logger.info(\"MPT EmbeddingFnMixinã‚’é©ç”¨\")\n",
    "   \n",
    "# convert LM to FlamingoLM\n",
    "extend_instance(lang_encoder, FlamingoLMMixin)\n",
    "logger.info(\"FlamingoLMMixinã‚’é©ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "__KNOWN_DECODER_LAYERS_ATTR_NAMES = {\n",
    "    \"opt\": \"model.decoder.layers\",\n",
    "    \"gptj\": \"transformer.h\",\n",
    "    \"gpt-j\": \"transformer.h\",\n",
    "    \"pythia\": \"gpt_neox.layers\",\n",
    "    \"llama\": \"model.layers\",\n",
    "    \"gptneoxforcausallm\": \"gpt_neox.layers\",\n",
    "    \"mpt\": \"transformer.blocks\",\n",
    "    \"mosaicgpt\": \"transformer.blocks\",\n",
    "}\n",
    "\n",
    "def _infer_decoder_layers_attr_name(model):\n",
    "    for k in __KNOWN_DECODER_LAYERS_ATTR_NAMES:\n",
    "        if k.lower() in model.__class__.__name__.lower():\n",
    "            return __KNOWN_DECODER_LAYERS_ATTR_NAMES[k]\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.\"\n",
    "    )\n",
    "\n",
    "if decoder_layers_attr_name is None:\n",
    "    decoder_layers_attr_name = _infer_decoder_layers_attr_name(lang_encoder)\n",
    "    logger.info(f\"ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å±æ€§åã‚’æ¨æ¸¬ {decoder_layers_attr_name=}\")\n",
    "\n",
    "lang_encoder.set_decoder_layers_attr_name(decoder_layers_attr_name)\n",
    "logger.info(f\"ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å±æ€§åã‚’è¨­å®š {decoder_layers_attr_name=}\")\n",
    "\n",
    "lang_encoder.resize_token_embeddings(len(text_tokenizer))\n",
    "logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã‚’ãƒªã‚µã‚¤ã‚º {len(text_tokenizer)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Flamingo(\n",
    "    vision_encoder,\n",
    "    lang_encoder,\n",
    "    text_tokenizer.encode(\"<|endofchunk|>\")[-1],\n",
    "    text_tokenizer.encode(\"<image>\")[-1],\n",
    "    vis_dim=open_clip.get_model_config(clip_vision_encoder_path)[\"vision_cfg\"][\n",
    "        \"width\"\n",
    "    ],\n",
    "    cross_attn_every_n_layers=cross_attn_every_n_layers,\n",
    "    # **flamingo_kwargs,\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Flamingoãƒ¢ãƒ‡ãƒ«ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209153f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.requires_grad_(False)\n",
    "assert sum(p.numel() for p in model.parameters() if p.requires_grad) == 0\n",
    "logger.info(\"ãƒ¢ãƒ‡ãƒ«ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ãƒªãƒ¼ã‚º\")\n",
    "\n",
    "model.perceiver.requires_grad_(True)\n",
    "logger.info(\"Perceiverã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¢ãƒ³ãƒ•ãƒªãƒ¼ã‚º\")\n",
    "\n",
    "model.lang_encoder.gated_cross_attn_layers.requires_grad_(True)\n",
    "logger.info(\"Gated Cross Attention Layersã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¢ãƒ³ãƒ•ãƒªãƒ¼ã‚º\")\n",
    "\n",
    "if not freeze_lm_embeddings:\n",
    "    model.lang_encoder.get_input_embeddings().requires_grad_(True)\n",
    "    logger.info(\"LM Input Embeddingsã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¢ãƒ³ãƒ•ãƒªãƒ¼ã‚º\")\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"è¨“ç·´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´æ¸ˆã¿ã®Flamingoãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852578b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "demo_image_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32100954",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "demo_image_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "query_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”»åƒã‚’å‰å‡¦ç†ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "logger.info(f\"ç”»åƒã‚’å‰å‡¦ç† {vision_x[0].shape=}, {vision_x[0].dtype=}\")\n",
    "\n",
    "# (num_media, channels, height, width)\n",
    "# (3, 3, 224, 224)\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "logger.info(f\"ç”»åƒã‚’çµåˆ {vision_x.shape=}, {vision_x.dtype=}\")\n",
    "\n",
    "# (batch_size, num_media, num_frames, channels, height, width)\n",
    "# (3, 3, 224, 224) -> (1, 3, 1, 3, 224, 224)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "logger.info(f\"ç”»åƒã‚’ãƒªã‚·ã‚§ã‚¤ãƒ— {vision_x.shape=}, {vision_x.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†\n",
    "\n",
    "# ç”Ÿæˆã®ãŸã‚ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å·¦å´ã«è¨­å®š\n",
    "text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "# <image>ã¨<|endofchunk|>ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "text_list = [\"<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of\"]\n",
    "logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ {text_list=}\")\n",
    "\n",
    "lang_x = text_tokenizer(text_list, return_tensors=\"pt\",)\n",
    "logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç† {lang_x['input_ids']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd30508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº† {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd823c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = text_tokenizer.decode(generated_text[0])\n",
    "logger.info(f\"ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
